#!/usr/bin/env python3
"""
Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð° Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ¾Ð² ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¹ Ð¸Ð· 1C:Ð¦Ð› (OData) Ð¿Ñ€ÑÐ¼Ð¾ Ð² cons.cons_redate.

Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº: InformationRegister_Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸ÑÐŸÐµÑ€ÐµÐ½Ð¾ÑÐ°ÐšÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸
ÐžÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸:
- Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ¾Ð²
- Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚ Ð²ÐµÐ´ÐµÑ‚ÑÑ Ð¿Ð¾ Ð´Ð°Ñ‚Ðµ Period (Ñ‡ÐµÑ€ÐµÐ· sys.sync_state)
- Ð¿Ñ€Ð¸ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ð¸ Ð½Ð¾Ð²Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¿Ð¾Ð»Ñ redate / redate_time Ð² cons.cons
- Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ Ð¾ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ°Ñ… Ð² Chatwoot
- ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð´Ð°Ñ‚Ñ‹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² 1C:Ð¦Ð›
"""
from __future__ import annotations

import os
import sys
import asyncio
import logging
import time
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Any, List
from urllib.parse import quote

import requests
from sqlalchemy import select, text, update
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ñ€ÐµÐ½ÑŒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð³Ð¾ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð°
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

from FastAPI.config import settings
from FastAPI.models import ConsRedate, Consultation, User
from FastAPI.services.chatwoot_client import ChatwootClient
from FastAPI.services.onec_client import OneCClient
from FastAPI.utils.etl_logging import ETLLogger

LOG_LEVEL = os.getenv("ETL_LOG_LEVEL", "INFO")
INITIAL_FROM_DATE = os.getenv("ETL_REDATE_INITIAL_FROM", "2025-12-01")
PAGE_SIZE = int(os.getenv("ODATA_PAGE_SIZE", "1000"))
MAX_ERROR_LOGS = int(os.getenv("ETL_MAX_ERROR_LOGS", "5"))

ENTITY = "InformationRegister_Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸ÑÐŸÐµÑ€ÐµÐ½Ð¾ÑÐ°ÐšÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸"

logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("pull_cons_redate")

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ OData
ODATA_BASEURL = settings.ODATA_BASEURL_CL or os.getenv("ODATA_BASEURL_CL")
ODATA_USER = settings.ODATA_USER
ODATA_PASSWORD = settings.ODATA_PASSWORD

# Database URL
DATABASE_URL = (
    f"postgresql+asyncpg://{settings.DB_USER}:{settings.DB_PASS}"
    f"@{settings.DB_HOST}:{settings.DB_PORT}/{settings.DB_NAME}"
)

HEADERS = {
    "User-Agent": "cons-middleware/redate-loader",
    "Accept": "application/json",
}


def clean_uuid(value: Optional[str]) -> Optional[str]:
    if not value or value == "00000000-0000-0000-0000-000000000000":
        return None
    return value


def clean_datetime(value: Optional[str]) -> Optional[datetime]:
    """ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð² datetime Ñ timezone (offset-aware)"""
    if not value or value.startswith("0001-01-01"):
        return None
    try:
        dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
        # Ð’ÐÐ–ÐÐž: Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ datetime Ð¸Ð¼ÐµÐµÑ‚ timezone (offset-aware)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        logger.warning("Failed to parse datetime: %s", value)
        return None


def http_get_with_backoff(url: str, auth: tuple, max_retries: int = 6, timeout: int = 120) -> requests.Response:
    session = requests.Session()
    attempt = 0
    while True:
        try:
            resp = session.get(url, auth=auth, headers=HEADERS, timeout=timeout)
            if resp.status_code in (429, 502, 503, 504):
                if attempt >= max_retries:
                    resp.raise_for_status()
                wait = min(2 ** attempt, 60)
                logger.warning("HTTP %s â€” retry in %s sec (attempt=%s)", resp.status_code, wait, attempt + 1)
                time.sleep(wait)
                attempt += 1
                continue
            resp.raise_for_status()
            return resp
        except requests.RequestException as exc:
            if attempt >= max_retries:
                logger.error("HTTP error after %s attempts: %s", attempt + 1, exc)
                raise
            wait = min(2 ** attempt, 60)
            logger.warning("Request failed: %s â€” retry in %s sec (attempt=%s)", exc, wait, attempt + 1)
            time.sleep(wait)
            attempt += 1


async def get_last_sync_date(db: AsyncSession) -> Optional[datetime]:
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ñƒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸, ÑƒÐ±ÐµÐ´Ð¸Ð²ÑˆÐ¸ÑÑŒ Ñ‡Ñ‚Ð¾ Ð¾Ð½Ð° offset-aware"""
    result = await db.execute(
        text("SELECT last_synced_at FROM sys.sync_state WHERE entity_name = :entity"),
        {"entity": ENTITY},
    )
    row = result.first()
    if row and row[0]:
        dt = row[0]
        # Ð’ÐÐ–ÐÐž: Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ datetime Ð¸Ð¼ÐµÐµÑ‚ timezone (offset-aware)
        if isinstance(dt, datetime) and dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    return None


async def save_sync_date(db: AsyncSession, value: datetime):
    await db.execute(
        text(
            """
            INSERT INTO sys.sync_state (entity_name, last_synced_at)
            VALUES (:entity, :date)
            ON CONFLICT (entity_name) DO UPDATE SET last_synced_at = EXCLUDED.last_synced_at
            """
        ),
        {"entity": ENTITY, "date": value},
    )


async def ensure_support_objects():
    """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ð¸Ð½Ð´ÐµÐºÑÑ‹/Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¿ÐµÑ€ÐµÐ´ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¾Ð¹."""
    engine = create_async_engine(
        DATABASE_URL,
        echo=False,
        pool_size=1,
        max_overflow=1,
        pool_pre_ping=True
    )
    async with engine.begin() as conn:
        # sync_state ÑƒÐ¶Ðµ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ Ð² init_db, Ð½Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð° Ð²ÑÑÐºÐ¸Ð¹ ÑÐ»ÑƒÑ‡Ð°Ð¹
        await conn.execute(
            text(
                """
                CREATE TABLE IF NOT EXISTS sys.sync_state (
                    entity_name TEXT PRIMARY KEY,
                    last_synced_at TIMESTAMPTZ
                )
                """
            )
        )
        await conn.execute(
            text(
                """
                CREATE UNIQUE INDEX IF NOT EXISTS uq_cons_redate_keys
                ON cons.cons_redate (cons_key, clients_key, manager_key, period)
                """
            )
        )
    await engine.dispose()


async def upsert_redate_rows(db: AsyncSession, rows: List[Dict[str, Any]]) -> Set[tuple]:
    """Ð’ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ¾Ð² Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð¾ ÐºÐ»ÑŽÑ‡ÐµÐ¹ Ð½Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"""
    if not rows:
        return set()
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐºÐ°ÐºÐ¸Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‚
    existing_keys = set()
    for row in rows:
        result = await db.execute(
            select(ConsRedate).where(
                ConsRedate.cons_key == row["cons_key"],
                ConsRedate.clients_key == row["clients_key"],
                ConsRedate.manager_key == row["manager_key"],
                ConsRedate.period == row["period"]
            ).limit(1)
        )
        if result.scalar_one_or_none():
            existing_keys.add((row["cons_key"], row["clients_key"], row["manager_key"], row["period"]))
    
    # Ð’ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð¾Ð²Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸
    new_rows = [row for row in rows if (row["cons_key"], row["clients_key"], row["manager_key"], row["period"]) not in existing_keys]
    if new_rows:
        stmt = insert(ConsRedate).values(new_rows)
        stmt = stmt.on_conflict_do_nothing(constraint="uq_cons_redate_keys")
        await db.execute(stmt)
    
    # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÐºÐ»ÑŽÑ‡Ð¸ Ð½Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ð¹
    return {(row["cons_key"], row["clients_key"], row["manager_key"], row["period"]) for row in new_rows}


async def update_consultation_schedule(
    db: AsyncSession,
    cons_key: str,
    new_date: Optional[datetime],
):
    if not new_date or not cons_key:
        return
    await db.execute(
        update(Consultation)
        .where(Consultation.cl_ref_key == cons_key)
        .values(
            redate=new_date.date(),
            redate_time=new_date.time(),
            updated_at=datetime.now(timezone.utc),
        )
    )


async def notify_chatwoot_redate(
    cons_id: str,
    old_date: Optional[datetime],
    new_date: Optional[datetime],
    manager_key: Optional[str] = None,
    db: Optional[AsyncSession] = None,
):
    """
    ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ Ð¾ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐµ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸ Ð² Chatwoot (ÐºÐ°Ðº note).
    
    ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð½Ðµ Ð±Ñ‹Ð»Ð¾ Ð»Ð¸ ÑƒÐ¶Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.
    """
    if not cons_id or cons_id.startswith(("temp_", "cl_")):
        # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ID
        return
    
    if not new_date:
        return
    
    # Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ ÑƒÑ‚Ð¸Ð»Ð¸Ñ‚Ñƒ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    from ..utils.notification_helpers import check_and_log_notification
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð½Ðµ Ð±Ñ‹Ð»Ð¾ Ð»Ð¸ ÑƒÐ¶Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ
    if db:
        # Ð’ÐÐ–ÐÐž: ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ manager_key Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ…ÐµÑˆÐ° (None -> "")
        # Ð­Ñ‚Ð¾ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ…ÐµÑˆÐ¸ Ð´Ð»Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¸ Ñ‚Ð¾Ð³Ð¾ Ð¶Ðµ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ°
        normalized_manager_key = manager_key if manager_key else ""
        notification_data = {
            "old_date": old_date.isoformat() if old_date else None,
            "new_date": new_date.isoformat(),
            "manager_key": normalized_manager_key  # Ð’ÑÐµÐ³Ð´Ð° ÑÑ‚Ñ€Ð¾ÐºÐ°, Ð½Ðµ None
        }
        # Ð’ÐÐ–ÐÐž: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½ÑƒÑŽ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑŽ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ NotificationLog,
        # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð½Ðµ Ð¿Ð¾Ñ‚ÐµÑ€ÑÐ»Ð°ÑÑŒ Ð¿Ñ€Ð¸ rollback Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ ETL
        already_sent = await check_and_log_notification(
            db=db,
            notification_type="redate",
            entity_id=cons_id,
            data=notification_data,
            use_separate_transaction=True  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½ÑƒÑŽ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑŽ Ð´Ð»Ñ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸
        )
        if already_sent:
            logger.debug(f"Redate notification already sent for cons_id={cons_id}, skipping")
            return
    
    try:
        chatwoot_client = ChatwootClient()
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¤Ð˜Ðž Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° Ð¸Ð· Ð‘Ð”
        manager_name = None
        if manager_key and db:
            try:
                manager_result = await db.execute(
                    select(User.description)
                    .where(User.cl_ref_key == manager_key)
                    .where(User.deletion_mark == False)
                    .limit(1)
                )
                manager_name = manager_result.scalar_one_or_none()
            except Exception as e:
                logger.warning(f"Failed to get manager name for {manager_key}: {e}")
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¾ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐµ
        old_date_str = old_date.strftime("%d.%m.%Y %H:%M") if old_date else "Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½Ð°"
        new_date_str = new_date.strftime("%d.%m.%Y %H:%M")
        
        message = f"ðŸ“… ÐšÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ñ Ð¿ÐµÑ€ÐµÐ½ÐµÑÐµÐ½Ð°\n"
        message += f"Ð¡Ñ‚Ð°Ñ€Ð°Ñ Ð´Ð°Ñ‚Ð°: {old_date_str}\n"
        message += f"ÐÐ¾Ð²Ð°Ñ Ð´Ð°Ñ‚Ð°: {new_date_str}"
        if manager_name:
            message += f"\nÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€: {manager_name}"
        elif manager_key:
            # Fallback Ð½Ð° UUID, ÐµÑÐ»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¤Ð˜Ðž
            message += f"\nÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€: {manager_key[:8]}..."
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ send_message Ð²Ð¼ÐµÑÑ‚Ð¾ send_note, Ñ‚Ð°Ðº ÐºÐ°Ðº note ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð½Ðµ Ð²Ð¸Ð´Ð½Ñ‹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñƒ
        await chatwoot_client.send_message(
            conversation_id=cons_id,
            content=message,
            message_type="outgoing"
        )
        
        # Ð£Ð²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ¶Ðµ Ð·Ð°Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð² check_and_log_notification
        
        logger.info(f"Sent redate message to Chatwoot for cons_id={cons_id}")
    except Exception as e:
        logger.warning(f"Failed to notify Chatwoot about redate (cons_id={cons_id}): {e}")


async def update_cl_consultation_date(
    cl_ref_key: str,
    new_date: Optional[datetime],
):
    """ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ñ‚Ñ‹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸ Ð² 1C:Ð¦Ð› Ñ‡ÐµÑ€ÐµÐ· OData"""
    if not cl_ref_key or not new_date:
        return
    
    try:
        onec_client = OneCClient()
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð´Ð°Ñ‚Ñƒ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸ Ð² Ð¦Ð›
        await onec_client.update_consultation_odata(
            ref_key=cl_ref_key,
            start_date=new_date,
        )
        logger.debug(f"[pull_cons_redate_cl] Updated consultation date in CL for cl_ref_key={cl_ref_key[:20]}, new_date={new_date}")
    except Exception as e:
        logger.warning(f"Failed to update CL consultation date (cl_ref_key={cl_ref_key}): {e}")


async def process_batch(db: AsyncSession, batch: List[Dict[str, Any]]):
    rows: List[Dict[str, Any]] = []
    latest_period: Optional[datetime] = None
    for item in batch:
        cons_key = clean_uuid(item.get("Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ÐžÐ±Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ_Key"))
        client_key = clean_uuid(item.get("ÐÐ±Ð¾Ð½ÐµÐ½Ñ‚_Key"))
        manager_key = clean_uuid(item.get("ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€_Key"))
        period_dt = clean_datetime(item.get("Period"))

        if not (cons_key and period_dt):
            continue

        row = {
            "cons_key": cons_key,
            "clients_key": client_key,
            "manager_key": manager_key,
            "period": period_dt,
            "old_date": clean_datetime(item.get("Ð¡Ñ‚Ð°Ñ€Ð°ÑÐ”Ð°Ñ‚Ð°")),
            "new_date": clean_datetime(item.get("ÐÐ¾Ð²Ð°ÑÐ”Ð°Ñ‚Ð°")),
        }
        rows.append(row)
        # Ð’ÐÐ–ÐÐž: Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð¾Ð±Ð° datetime Ð¸Ð¼ÐµÑŽÑ‚ timezone Ð¿ÐµÑ€ÐµÐ´ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÐµÐ¼
        if latest_period is None:
            latest_period = period_dt
        elif period_dt:
            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¾Ð±Ð° datetime Ðº UTC Ð´Ð»Ñ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ
            if period_dt.tzinfo is None:
                period_dt = period_dt.replace(tzinfo=timezone.utc)
            if latest_period.tzinfo is None:
                latest_period = latest_period.replace(tzinfo=timezone.utc)
            if period_dt > latest_period:
                latest_period = period_dt

    if rows:
        # Ð’ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ»ÑŽÑ‡Ð¸ Ð½Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
        new_keys = await upsert_redate_rows(db, rows)
        
        for row in rows:
            row_key = (row["cons_key"], row["clients_key"], row["manager_key"], row["period"])
            cons_key = row["cons_key"]
            old_date = row["old_date"]
            new_date = row["new_date"]
            manager_key = row["manager_key"]
            
            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð² Ð‘Ð”
            await update_consultation_schedule(db, cons_key, new_date)
            
            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
            # Ð’ÐÐ–ÐÐž: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ notify_chatwoot_redate
            # Ð­Ñ‚Ð¾ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÑƒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð² cons_redate ÐµÑ‰Ðµ Ð½Ðµ Ð² Ð‘Ð”
            if row_key in new_keys:
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸ Ð² Chatwoot Ð¸ Ð¦Ð›
                result = await db.execute(
                    select(Consultation).where(Consultation.cl_ref_key == cons_key)
                )
                consultation = result.scalar_one_or_none()
                
                if consultation:
                    # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ Ð² Chatwoot (Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸)
                    if consultation.cons_id:
                        await notify_chatwoot_redate(
                            consultation.cons_id, old_date, new_date, manager_key, db=db
                        )
                    
                    # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð´Ð°Ñ‚Ñƒ Ð² Ð¦Ð› (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ cl_ref_key)
                    if consultation.cl_ref_key:
                        await update_cl_consultation_date(consultation.cl_ref_key, new_date)
    
    return len(rows), latest_period


async def pull_cons_redate():
    etl_logger = ETLLogger("pull_cons_redate_cl", ENTITY)
    
    if not (ODATA_BASEURL and ODATA_USER and ODATA_PASSWORD):
        etl_logger.critical_error("ODATA config missing. Check ODATA_BASEURL_CL, ODATA_USER, ODATA_PASSWORD")
        sys.exit(1)

    etl_logger.start({
        "ODATA_BASEURL": ODATA_BASEURL,
        "ENTITY": ENTITY,
        "INITIAL_FROM_DATE": INITIAL_FROM_DATE,
        "PAGE_SIZE": PAGE_SIZE
    })

    auth = (ODATA_USER, ODATA_PASSWORD)
    # Ð’ÐÐ–ÐÐž: ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ð¿ÑƒÐ» ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ETL ÑÐºÑ€Ð¸Ð¿Ñ‚Ð°
    engine = create_async_engine(
        DATABASE_URL,
        echo=False,
        pool_size=2,
        max_overflow=2,
        pool_pre_ping=True,
        pool_recycle=3600,
        pool_timeout=30
    )
    AsyncSessionLocal = async_sessionmaker(engine, expire_on_commit=False)

    try:
        async with AsyncSessionLocal() as db:
            last_sync = await get_last_sync_date(db)
            if last_sync:
                from_dt = last_sync - timedelta(hours=6)
                from_date = from_dt.strftime("%Y-%m-%dT%H:%M:%S")
                etl_logger.sync_info(last_sync, from_date, buffer_days=None)  # Ð‘ÑƒÑ„ÐµÑ€ Ð² Ñ‡Ð°ÑÐ°Ñ…
            else:
                from_date = f"{INITIAL_FROM_DATE}T00:00:00"
                etl_logger.sync_info(None, from_date)

            skip = 0
            error_logs = 0
            last_period_processed: Optional[datetime] = last_sync

            while True:
                batch_num = skip // PAGE_SIZE + 1
                filter_part = f"Period ge datetime'{from_date}'"
                url = (
                    f"{ODATA_BASEURL}{ENTITY}?$format=json"
                    f"&$filter={quote(filter_part)}"
                    f"&$orderby=Period asc"
                    f"&$top={PAGE_SIZE}&$skip={skip}"
                )

                etl_logger.batch_start(batch_num, skip, PAGE_SIZE)

                try:
                    resp = http_get_with_backoff(url, auth)
                except Exception as exc:
                    etl_logger.batch_error(batch_num, exc, skip)
                    break

                batch = resp.json().get("value", [])
                if not batch:
                    break

                try:
                    processed, latest_period = await process_batch(db, batch)
                    if latest_period:
                        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¾Ð±Ð° datetime Ðº UTC Ð¿ÐµÑ€ÐµÐ´ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÐµÐ¼
                        if latest_period.tzinfo is None:
                            latest_period = latest_period.replace(tzinfo=timezone.utc)
                        if last_period_processed is None:
                            last_period_processed = latest_period
                        else:
                            if last_period_processed.tzinfo is None:
                                last_period_processed = last_period_processed.replace(tzinfo=timezone.utc)
                            if latest_period > last_period_processed:
                                last_period_processed = latest_period
                except Exception as exc:
                    if error_logs < MAX_ERROR_LOGS:
                        etl_logger.batch_error(batch_num, exc, skip)
                    elif error_logs == MAX_ERROR_LOGS:
                        logger.warning("[pull_cons_redate_cl] Further redate processing errors suppressed")
                    error_logs += 1
                    await db.rollback()
                    break

                await db.commit()

                # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð±Ð°Ñ‚Ñ‡Ð°
                etl_logger.batch_progress(batch_num, len(batch), created=processed, updated=0, errors=0)

                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ sync_state Ð¿Ð¾ÑÐ»Ðµ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð°
                if last_period_processed:
                    try:
                        await save_sync_date(db, last_period_processed)
                        await db.commit()
                        etl_logger.sync_state_saved(last_period_processed, batch_num)
                    except Exception as sync_error:
                        logger.warning(f"[pull_cons_redate_cl] Failed to save sync state after batch: {sync_error}")

                if len(batch) < PAGE_SIZE:
                    break
                skip += PAGE_SIZE

            # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ñ‚Ñ‹ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸
            if last_period_processed:
                await save_sync_date(db, last_period_processed)
                await db.commit()
                etl_logger.sync_state_saved(last_period_processed)
            
            etl_logger.finish(success=True)
    except Exception as e:
        etl_logger.finish(success=False, error=e)
        sys.exit(1)
    finally:
        await engine.dispose()


if __name__ == "__main__":
    asyncio.run(ensure_support_objects())
    asyncio.run(pull_cons_redate())

